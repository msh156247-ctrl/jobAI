/**
 * ì›¹ ìŠ¤í¬ë ˆì´í•‘ ìœ í‹¸ë¦¬í‹°
 *
 * ì‹¤ì œ ì±„ìš© ì‚¬ì´íŠ¸ì—ì„œ ê³µê³ ë¥¼ í¬ë¡¤ë§í•˜ê¸° ìœ„í•œ ìŠ¤í¬ë ˆì´í¼
 *
 * ì£¼ì˜ì‚¬í•­:
 * 1. ì‹¤ì œ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” Puppeteerë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤
 * 2. ê° ì‚¬ì´íŠ¸ì˜ robots.txtì™€ ì´ìš©ì•½ê´€ì„ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤
 * 3. Rate limitingì„ ì ìš©í•˜ì—¬ ì„œë²„ì— ë¶€ë‹´ì„ ì£¼ì§€ ì•Šì•„ì•¼ í•©ë‹ˆë‹¤
 * 4. ì‚¬ì´íŠ¸ êµ¬ì¡°ê°€ ë³€ê²½ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì •ê¸°ì ì¸ ì—…ë°ì´íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤
 */

// íƒ€ì… ì •ì˜ëŠ” ë³„ë„ íŒŒì¼ì—ì„œ import (í´ë¼ì´ì–¸íŠ¸ ë²ˆë“¤ë§ ë°©ì§€)
export type { ScraperParams, ScrapedJob } from './types'
import type { ScraperParams, ScrapedJob } from './types'

/**
 * ì‚¬ëŒì¸ ìŠ¤í¬ë ˆì´í¼
 */
export async function scrapeSaramin(params: ScraperParams): Promise<ScrapedJob[]> {
  const { crawlSaramin } = await import('../crawling/saraminCrawler')
  return crawlSaramin(params)
}

/**
 * ì¡ì½”ë¦¬ì•„ ìŠ¤í¬ë ˆì´í¼
 */
export async function scrapeJobKorea(params: ScraperParams): Promise<ScrapedJob[]> {
  const { crawlJobKorea } = await import('../crawling/jobkoreaCrawler')
  return crawlJobKorea(params)
}

/**
 * ì›í‹°ë“œ ìŠ¤í¬ë ˆì´í¼
 */
export async function scrapeWanted(params: ScraperParams): Promise<ScrapedJob[]> {
  const { crawlWanted } = await import('../crawling/wantedCrawler')
  return crawlWanted(params)
}

/**
 * ì¸í¬ë£¨íŠ¸ ìŠ¤í¬ë ˆì´í¼
 */
export async function scrapeIncruit(params: ScraperParams): Promise<ScrapedJob[]> {
  const { crawlIncruit } = await import('../crawling/incruitCrawler')
  return crawlIncruit(params)
}

/**
 * ì¡í”Œë˜ë‹› ìŠ¤í¬ë ˆì´í¼
 */
export async function scrapeJobPlanet(params: ScraperParams): Promise<ScrapedJob[]> {
  const { crawlJobPlanet } = await import('../crawling/jobplanetCrawler')
  return crawlJobPlanet(params)
}

// ============================================================================
// URL ë¹Œë” í•¨ìˆ˜ë“¤
// ============================================================================

function buildSaraminSearchUrl(params: ScraperParams): string {
  const baseUrl = 'https://www.saramin.co.kr/zf_user/search'
  const searchParams = new URLSearchParams()

  if (params.keyword) searchParams.append('searchword', params.keyword)

  // ì§€ì—­ ì½”ë“œ ë§¤í•‘
  if (params.location) {
    const locationCodes: Record<string, string> = {
      'ì„œìš¸': '101000',
      'ê²½ê¸°': '102000',
      'ì¸ì²œ': '108000',
      'ë¶€ì‚°': '106000',
      'ëŒ€êµ¬': '104000',
      'ëŒ€ì „': '105000',
      'ê´‘ì£¼': '103000',
      'ìš¸ì‚°': '107000',
      'ì„¸ì¢…': '118000',
      'ê°•ì›': '109000',
      'ì¶©ë¶': '111000',
      'ì¶©ë‚¨': '110000',
      'ì „ë¶': '112000',
      'ì „ë‚¨': '113000',
      'ê²½ë¶': '114000',
      'ê²½ë‚¨': '115000',
      'ì œì£¼': '116000'
    }
    const code = locationCodes[params.location]
    if (code) searchParams.append('loc_mcd', code)
  }

  // ê²½ë ¥
  if (params.minExperience !== undefined || params.maxExperience !== undefined) {
    const min = params.minExperience || 0
    const max = params.maxExperience || 99
    searchParams.append('exp_cd', `${min},${max}`)
  }

  // ì—°ë´‰
  if (params.minSalary) {
    searchParams.append('sal_type', '1')
    searchParams.append('sal', String(params.minSalary * 10000))
  }

  // ê·¼ë¬´ í˜•íƒœ
  if (params.employmentType) {
    const typeCodes: Record<string, string> = {
      'onsite': '1',
      'remote': '4',
      'dispatch': '3'
    }
    const code = typeCodes[params.employmentType]
    if (code) searchParams.append('job_type', code)
  }

  return `${baseUrl}?${searchParams.toString()}`
}

function buildJobKoreaSearchUrl(params: ScraperParams): string {
  const baseUrl = 'https://www.jobkorea.co.kr/Search/'
  const searchParams = new URLSearchParams()

  if (params.keyword) searchParams.append('stext', params.keyword)

  // ì§€ì—­
  if (params.location) {
    const locationCodes: Record<string, string> = {
      'ì„œìš¸': '1',
      'ê²½ê¸°': '2',
      'ì¸ì²œ': '3',
      'ë¶€ì‚°': '4',
      'ëŒ€êµ¬': '5',
      'ëŒ€ì „': '6',
      'ê´‘ì£¼': '7',
      'ìš¸ì‚°': '8',
      'ì„¸ì¢…': '9',
      'ê°•ì›': '10',
      'ì¶©ë¶': '11',
      'ì¶©ë‚¨': '12',
      'ì „ë¶': '13',
      'ì „ë‚¨': '14',
      'ê²½ë¶': '15',
      'ê²½ë‚¨': '16',
      'ì œì£¼': '17'
    }
    const code = locationCodes[params.location]
    if (code) searchParams.append('local', code)
  }

  // ê²½ë ¥
  if (params.minExperience !== undefined) {
    searchParams.append('exp_min', String(params.minExperience))
  }
  if (params.maxExperience !== undefined) {
    searchParams.append('exp_max', String(params.maxExperience))
  }

  return `${baseUrl}?${searchParams.toString()}`
}

function buildWantedSearchUrl(params: ScraperParams): string {
  const baseUrl = 'https://www.wanted.co.kr/search'
  const searchParams = new URLSearchParams()

  if (params.keyword) searchParams.append('query', params.keyword)

  // ì§€ì—­
  if (params.location) {
    const locationTags: Record<string, string> = {
      'ì„œìš¸': 'locations.seoul',
      'ê²½ê¸°': 'locations.gyeonggi',
      'ë¶€ì‚°': 'locations.busan',
      'ëŒ€êµ¬': 'locations.daegu',
      'ì¸ì²œ': 'locations.incheon',
      'ê´‘ì£¼': 'locations.gwangju',
      'ëŒ€ì „': 'locations.daejeon',
      'ìš¸ì‚°': 'locations.ulsan'
    }
    const tag = locationTags[params.location]
    if (tag) searchParams.append('tag_type_ids[]', tag)
  }

  // ê²½ë ¥
  if (params.minExperience !== undefined) {
    searchParams.append('years', String(params.minExperience))
  }

  return `${baseUrl}?${searchParams.toString()}`
}

function buildIncruitSearchUrl(params: ScraperParams): string {
  const baseUrl = 'https://www.incruit.com/job/search.asp'
  const searchParams = new URLSearchParams()

  if (params.keyword) searchParams.append('keyword', params.keyword)
  if (params.location) searchParams.append('region', params.location)

  // ê²½ë ¥
  if (params.minExperience !== undefined || params.maxExperience !== undefined) {
    const min = params.minExperience || 0
    const max = params.maxExperience || 99
    searchParams.append('career', `${min}-${max}`)
  }

  return `${baseUrl}?${searchParams.toString()}`
}

function buildJobPlanetSearchUrl(params: ScraperParams): string {
  const baseUrl = 'https://www.jobplanet.co.kr/job_postings/search'
  const searchParams = new URLSearchParams()

  if (params.keyword) searchParams.append('query', params.keyword)
  if (params.location) searchParams.append('location', params.location)

  // ê²½ë ¥
  if (params.minExperience !== undefined) {
    searchParams.append('career_min', String(params.minExperience))
  }
  if (params.maxExperience !== undefined) {
    searchParams.append('career_max', String(params.maxExperience))
  }

  return `${baseUrl}?${searchParams.toString()}`
}

// ============================================================================
// ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
// ============================================================================

/**
 * Rate limitingì„ ìœ„í•œ ë”œë ˆì´ í•¨ìˆ˜
 */
export function delay(ms: number): Promise<void> {
  return new Promise(resolve => setTimeout(resolve, ms))
}

/**
 * ì—¬ëŸ¬ ì‚¬ì´íŠ¸ë¥¼ ë³‘ë ¬ë¡œ í¬ë¡¤ë§
 */
export async function scrapeAllSites(
  params: ScraperParams,
  options: {
    validate?: boolean
    removeDuplicates?: boolean
  } = {}
): Promise<ScrapedJob[]> {
  const { validate = true, removeDuplicates: removeDups = true } = options

  const results = await Promise.allSettled([
    scrapeSaramin(params),
    scrapeJobKorea(params),
    scrapeWanted(params),
    scrapeIncruit(params),
    scrapeJobPlanet(params)
  ])

  const allJobs: ScrapedJob[] = []
  const errors: string[] = []

  results.forEach((result, index) => {
    const siteName = ['Saramin', 'JobKorea', 'Wanted', 'Incruit', 'JobPlanet'][index]

    if (result.status === 'fulfilled') {
      allJobs.push(...result.value)
      console.log(`[${siteName}] Successfully scraped ${result.value.length} jobs`)
    } else {
      errors.push(`[${siteName}] ${result.reason}`)
      console.error(`[${siteName}] Scraping failed:`, result.reason)
    }
  })

  if (errors.length > 0) {
    console.warn('Some scrapers failed:', errors)
  }

  let finalJobs = allJobs

  // ì¤‘ë³µ ì œê±°
  if (removeDups && allJobs.length > 0) {
    const { removeDuplicates } = await import('../crawling/validator')
    const result = removeDuplicates(allJobs)
    finalJobs = result.unique
    if (result.duplicates > 0) {
      console.log(`ğŸ”„ ì¤‘ë³µ ì œê±°: ${result.duplicates}ê±´`)
    }
  }

  // ê²€ì¦
  if (validate && finalJobs.length > 0) {
    const { validateJobs, printValidationReport } = await import('../crawling/validator')
    const report = validateJobs(finalJobs)
    printValidationReport(report)

    // ìœ íš¨í•œ ê³µê³ ë§Œ ë°˜í™˜
    const validJobIds = new Set(
      report.details.filter(d => d.valid).map(d => d.jobId)
    )
    finalJobs = finalJobs.filter(job => validJobIds.has(job.id))
  }

  return finalJobs
}
